{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitfastai07virtualenv62037e29330b498fa3124404daeda7a7",
   "display_name": "Python 3.7.3 64-bit ('fastai-0.7': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from the open source repo: https://github.com/sugey/pytorch-yolov3\n",
    "\n",
    "# Let's talk docs!!\n",
    "def predict_transform(prediction, inp_dim, anchors, num_classes, train=False):\n",
    "    \"\"\"\n",
    "    Takes a detection layer feature map and turns it into a 2D tensor where\n",
    "    every row is a list with objectness (P0) bounding box coordinates (Bx, By, Bw, Bh),\n",
    "    and class probabilities for each anchor.\n",
    "\n",
    "    Params:\n",
    "        prediction: tensor from prediction, Batch x Channels x Width x Height\n",
    "            example: 416 x 416 input, first YOLO detection prediction: torch.Size([1, 255, 13, 13])\n",
    "        inp_dim: input image dimensions\n",
    "        anchors: list of anchors\n",
    "        num_classes: number of classes for prediction. VOC is usually 20, COCO=80\n",
    "        CUDA: CUDA flag\n",
    "\n",
    "    Returns:\n",
    "        prediction for classes, for 13x13 is a torch.Size([1, 507, 85]), bs x (anchor * grid * grid) x (classes + bboxs + 1)\n",
    "    \"\"\"\n",
    "\n",
    "# In the actual code, we use this as a function, but we're going to break it up for explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction input size: torch.Size([1, 255, 13, 13])\n"
    }
   ],
   "source": [
    "#We're going to keep everything on CPU. Normally, we would push a lot of this to GPU with .cuda() or to_device(cuda)\n",
    "\n",
    "# Let's load the function params as dummy data\n",
    "tensor = torch.tensor((), dtype=torch.int32)\n",
    "prediction = tensor.new_ones((1, 255, 13, 13))\n",
    "inp_dim = 416\n",
    "anchors = [(116, 90), (156, 198), (373, 326)]\n",
    "num_classes = 80\n",
    "\n",
    "print(f\"Prediction input size: {prediction.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "batch_size: 1\nstride: 32\ngrid_size: 13\nbbox_attrs: 85\nnum_anchors: 3\nanchors: [(3.625, 2.8125), (4.875, 6.1875), (11.65625, 10.1875)]\n"
    }
   ],
   "source": [
    "\n",
    "# size of the current batch of the detection layer\n",
    "batch_size = prediction.size(0)\n",
    "# Determined by dividing the full image size by the width of the feature map,\n",
    "# which is also the size of the region maps. 416 x 416 = 13, 26, 52\n",
    "# for region map sizes among the 3 detection layers.\n",
    "stride = inp_dim // prediction.size(2)\n",
    "# This is the number of regions, so for an input image of\n",
    "# 416 x 416, goes in sizes of 13, 26, 52\n",
    "grid_size = inp_dim // stride\n",
    "# Boxes always have at least 5 slots, 1 for the objectness score (what is the\n",
    "# probability an object is in this grid box) and 4 dimensional attributes\n",
    "# of center coodinates + width + height or Bx, By, Bw, Bh\n",
    "bbox_attrs = 5 + num_classes\n",
    "# Obviously, number of anchor boxes we decided to use\n",
    "num_anchors = len(anchors)\n",
    "\n",
    "# We need to reduce the anchors from their size on the full image\n",
    "# to the reduced grid sizes, and resave them in a list.\n",
    "anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n",
    "\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "print(f\"stride: {stride}\")\n",
    "print(f\"grid_size: {grid_size}\")\n",
    "print(f\"bbox_attrs: {bbox_attrs}\")\n",
    "print(f\"num_anchors: {num_anchors}\")\n",
    "print(f\"anchors: {anchors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction Tensor size: torch.Size([1, 255, 169])\n"
    }
   ],
   "source": [
    "# We need to get the prediction map data down to a 2D tensor with tensor data turned into bounding boxes on 416 x 416 image, generates tensor of 1 deep (flat) x 255 x 169\n",
    "prediction = prediction.view(\n",
    "    batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n",
    "\n",
    "print(f\"Prediction Tensor size: {prediction.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction Tensor size: torch.Size([1, 169, 255])\n"
    }
   ],
   "source": [
    "# Generates tensor and rotates it to 1 x 169 x 255\n",
    "prediction = prediction.transpose(1, 2).contiguous()\n",
    "\n",
    "print(f\"Prediction Tensor size: {prediction.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction Tensor size: torch.Size([1, 507, 85])\n"
    }
   ],
   "source": [
    "# On the 416 image, creates a 3D tensor of 1 x 507 x 85. This accounts for every anchor in every grid region. In the case of a 0 threshold, all\n",
    "# of these boxes would appear on an image.\n",
    "prediction = prediction.view(\n",
    "    batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n",
    "\n",
    "print(f\"Prediction Tensor size: {prediction.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-24-96e45412e801>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-96e45412e801>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    return prediction\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# We stop here for training normally in the function.  Inferencing will require a few more steps, which we won't cover today since they're not related to re-shaping as much.\n",
    "if train:\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction Tensor size: torch.Size([507, 85])\n"
    }
   ],
   "source": [
    "# What if we wanted to shrink the torch.Size([1, 507, 85]) to a torch.Size([507, 85])\n",
    "# The parameter is the index shape to remove\n",
    "prediction_ = prediction.squeeze(0)\n",
    "print(f\"Prediction Tensor size: {prediction_.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction Tensor size: torch.Size([1, 507, 85])\n"
    }
   ],
   "source": [
    "# What if we wanted to expand the torch.Size([507, 85]) back to a torch.Size([1, 507, 85])\n",
    "# The parameter is the index shape to add\n",
    "prediction_1 = prediction_.unsqueeze(0)\n",
    "print(f\"Prediction Tensor size: {prediction_1.size()}\")"
   ]
  }
 ]
}